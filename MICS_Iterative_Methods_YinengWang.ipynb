{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab3_YinengWang.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johanhoffman/methods-in-computational-science/blob/main/MICS_Iterative_Methods_YinengWang.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RgtXlfYO_i7"
      },
      "source": [
        "# **Lab 3: Iterative Methods**\n",
        "**Yineng Wang**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x_J5FVuPzbm"
      },
      "source": [
        "# **Abstract**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UFTSzW7P8kL"
      },
      "source": [
        "This report implements four functions:\n",
        "\n",
        "1. Jacobi iteration\n",
        "2. Gauss-Seidel iteration\n",
        "3. Newton's method for scalar nonlinear equation\n",
        "4. Newton's method for vector nonlinear equation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkT8J7uOWpT3"
      },
      "source": [
        "#**About the code**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmB2noTr1Oyo"
      },
      "source": [
        "This report is written by Yineng Wang, based on Johan Hoffman's template."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pdll1Xc9WP0e"
      },
      "source": [
        "# Copyright (C) 2020,2021 Johan Hoffman (jhoffman@kth.se)\n",
        "# Copyright (C) 2021 Yineng Wang (yineng@kth.se)\n",
        "\n",
        "# This file is part of the course DD2363 Methods in Scientific Computing\n",
        "# KTH Royal Institute of Technology, Stockholm, Sweden\n",
        "#\n",
        "# This is free software: you can redistribute it and/or modify\n",
        "# it under the terms of the GNU Lesser General Public License as published by\n",
        "# the Free Software Foundation, either version 3 of the License, or\n",
        "# (at your option) any later version.\n",
        "\n",
        "# This template is maintained by Johan Hoffman\n",
        "# Please report problems to jhoffman@kth.se"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28xLGz8JX3Hh"
      },
      "source": [
        "# **Set up environment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2PYNusD08Wa"
      },
      "source": [
        "To have access to the neccessary modules you have to run this cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw7VlErAX7NS"
      },
      "source": [
        "import time\n",
        "import unittest\n",
        "\n",
        "import numpy as np\n",
        "from numpy.linalg import norm"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnO3lhAigLev"
      },
      "source": [
        "# **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5zMzgPlRAF6"
      },
      "source": [
        "Fixed point iteration is a performant method to solve equations. For a linear system of equations $Ax = b$, preconditioned Richardson iteration can be emplyoed, which takes the form\n",
        "\n",
        "$$x^{(k+1)} = (I - \\alpha B A) x^{(k)} + \\alpha B b$$\n",
        "\n",
        "or\n",
        "\n",
        "$$x^{(k+1)} = (I - \\alpha A B) x^{(k)} + \\alpha b$$\n",
        "\n",
        "By taking $\\alpha = 1$ and $B = D^{-1}$ in left preconditioned Richardson iteration, we obtain Jacobi iteration\n",
        "\n",
        "$$x^{(k+1)} = (I - D^{-1} A) x^{(k)} + D^{-1} b,$$\n",
        "\n",
        "where $D$ is a diagonal matrix whose entries are the diagonal elements of $A$. And by taking $\\alpha = 1$ and $B = L^{-1}$ in left preconditioned Richardson iteration, we obtain Gauss-Seidel iteration\n",
        "\n",
        "$$x^{(k+1)} = (I - L^{-1} A) x^{(k)} + L^{-1} b,$$\n",
        "\n",
        "where $L$ is a upper triangular matrix obtained from the matrix $A$ by zeroing out all entries above the diagonal.\n",
        "\n",
        "Jacobi iteration and Gauss-Seidel iteration are especially efficient when $A$ is diagonal dominant. There are more constrains on the design of Gauss-Seidel iteration than Jacobi iteration.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxF9haA8iC8Y"
      },
      "source": [
        "If the equation $f(x) = 0$ is non-linear, one can apply Newton's method to solve it, which takes the form\n",
        "\n",
        "$$x^{(k+1)} = x^{(k)} - f'(x^{(k)})^{-1} f(x^{(k)}).$$\n",
        "\n",
        "It can be shown that Newton's method has a quadratic order of convergence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOQvukXZq5U5"
      },
      "source": [
        "# **Method**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF4iBj5VURZx"
      },
      "source": [
        "The implementations are based on the lecture notes. NumPy's arrays are used to represent objects (vectors, matrices) of linear algebra."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhwl2QEm-wFn"
      },
      "source": [
        "## 1. Jacobi iteration for Ax=b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AToDrmSP-2CQ"
      },
      "source": [
        "Input: matrix A, vector b\n",
        "\n",
        "Output: vector x\n",
        "\n",
        "Jacobi iteration is a stationary iterative method taking the form of\n",
        "$x^{(k+1)} = (I - D^{-1} A) x^{(k)} + D^{-1} b$, where $D$ is a diagonal matrix whose entries are the diagonal elements of $A$. In terms of the components of the matrix $A=(a_{ij})$, we have\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "x_i^{(k+1)} &= (I - D^{-1} A) x^{(k)} + D^{-1} b \\\\\n",
        "&= x_i^{(k)} - a_{ii}^{-1} \\sum_{j=1}^{n} a_{ij} x_j^{(k)} + a_{ii}^{-1} b_i \\\\\n",
        "&= a_{ii}^{-1} \\left(b_i - \\sum_{j\\neq i} a_{ij} x_j^{(k)}\\right), \\forall i.\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "In the implementation, we make use of NumPy's vectorized operators to avoid loops. The stopping criteria is based on the formula $\\Vert r^{(k)} \\Vert / \\Vert b \\Vert < TOL$ in section (7.3) of the lecture notes. We do not formally check whether the iteration converges (for example, by using the strictly diagonally dominant condition), but instead set a upper bound for the number of iterations, `max_iter`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WonlVoTpNEvc"
      },
      "source": [
        "def jacobi_iteration(A, b, tol=1e-9, max_iter=2000):\n",
        "    n = b.shape[0]\n",
        "    d = A.diagonal()\n",
        "    if np.any(d == 0):\n",
        "        raise ValueError('there is a zero diagonal entry')\n",
        "    D_inv_A = A / d.reshape((n,1))    # Equivalent to D^-1 A\n",
        "    D_inv_b = b / d    # Equivalent to D^-1 b\n",
        "    norm_b = norm(b)\n",
        "\n",
        "    x = np.zeros((n,))\n",
        "    n_iter = 0\n",
        "    while True:\n",
        "        residual = A @ x - b\n",
        "        if norm(residual) / norm_b < tol:\n",
        "            return x\n",
        "        x = x - D_inv_A @ x + D_inv_b    # Equivalent to x = (I - D^-1 A) x + D^-1 b\n",
        "        n_iter += 1\n",
        "        if n_iter > max_iter:\n",
        "            raise ValueError('the iteration diverges')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I11edgKf-9WT"
      },
      "source": [
        "## 2. Gauss-Seidel iteration for Ax=b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dorgIwt_B9n"
      },
      "source": [
        "Input: matrix A, vector b\n",
        "\n",
        "Output: vector x\n",
        "\n",
        "Gauss-Seidel iteration is a stationary iterative method taking the form of\n",
        "$x^{(k+1)} = (I - L^{-1} A) x^{(k)} + L^{-1} b$, where $L$ is a upper triangular matrix obtained from the matrix $A$ by zeroing out all entries above the diagonal.\n",
        "\n",
        "Since $x^{(k+1)} = (I - L^{-1} A) x^{(k)} + L^{-1} b = x^{(k)} + L^{-1}(A x^{(k)} + b)$ and $L$ is upper triangular, we can solve get the value of $L^{-1}(-A x^{(k)} + b)$ by forward substitution (see algorithm (5.1) of the lecture notes). Denote $L^{-1}(-A x^{(k)} + b)$ by $y^{(k)}$, then we have\n",
        "\n",
        "$$\n",
        "y_i^{(k)} = a_{ii}^{-1}\\left(b_i - \\sum_{j=1}^n a_{ij} x_j^{(k)} - \\sum_{j<i} a_{ij} y_j^{(k)}\\right),\n",
        "$$\n",
        "\n",
        "and\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "x_i^{(k+1)} &= x_i^{(k)} + y_i^{(k)} \\\\\n",
        "&= x_i^{(k)} + a_{ii}^{-1}\\left(b_i - \\sum_{j=1}^n a_{ij} x_j^{(k)} - \\sum_{j<i} a_{ij} y_j^{(k)}\\right) \\\\\n",
        "&= a_{ii}^{-1} a_{ii} x_i^{(k)} + a_{ii}^{-1}\\left(b_i - \\sum_{j=1}^n a_{ij} x_j^{(k)} - \\sum_{j<i} a_{ij} (x_j^{(k+1)} - x_j^{(k)})\\right) \\\\\n",
        "&= a_{ii}^{-1} \\left(b_i - \\sum_{j>i} a_{ij} x_j^{(k)} - \\sum_{j<i} a_{ij} x_j^{(k+1)}\\right), \\forall i.\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Unlike Jacobi iteration, Gauss-Seidel iteration cannot be implemented by taking considerable advantage of NumPy's vectorization, but it's still worth a try."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5AxEFVqCgox"
      },
      "source": [
        "def gauss_seidel_iteration(A, b, tol=1e-9, max_iter=2000):\n",
        "    n = b.shape[0]\n",
        "    d = A.diagonal()\n",
        "    if np.any(d == 0):\n",
        "        raise ValueError('there is a zero diagonal entry')\n",
        "    norm_b = norm(b)\n",
        "\n",
        "    x = np.zeros((n,))\n",
        "    n_iter = 0\n",
        "    while True:\n",
        "        residual = A @ x - b\n",
        "        if norm(residual) / norm_b < tol:\n",
        "            return x\n",
        "        for i in range(n):\n",
        "            x[i] = (b[i] - A[i,:i] @ x[:i] - A[i,(i+1):] @ x[(i+1):]) / d[i]\n",
        "        n_iter += 1\n",
        "        if n_iter > max_iter:\n",
        "            raise ValueError('the iteration diverges')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrW4AVnn_Eff"
      },
      "source": [
        "## 3. Newton's method for scalar nonlinear equation f(x)=0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qibFnaAf_GF_"
      },
      "source": [
        "Input: scalar function f(x)\n",
        "\n",
        "Output: real number x\n",
        "\n",
        "For the equation $f(x) = 0$, Newton's method takes the form\n",
        "\n",
        "$$x^{(k+1)} = x^{(k)} - \\frac{f(x^{(k)})}{f'(x^{(k)})}.$$\n",
        "\n",
        "To get the derivative of $f(x)$, we use a finite difference approximation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hYwGcpiI2UX"
      },
      "source": [
        "def derivative(f, x, h):\n",
        "    return (f(x + h/2) - f(x - h/2)) / h\n",
        "\n",
        "\n",
        "def newton_method(f, tol=1e-9, max_iter=2000, x0=0, h=1e-6):\n",
        "    x = x0\n",
        "    n_iter = 0\n",
        "    while True:\n",
        "        f_x = f(x)\n",
        "        if abs(f_x) < tol:\n",
        "            return x\n",
        "        x -= f_x / derivative(f, x, h)\n",
        "        n_iter += 1\n",
        "        if n_iter > max_iter:\n",
        "            raise ValueError('the iteration diverges')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtGG8UjnI6NU"
      },
      "source": [
        "## 4. Newton's method for vector nonlinear equation f(x)=0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUJr6XSVI9Wn"
      },
      "source": [
        "Input: vector function f(x)\n",
        "\n",
        "Output: vector x\n",
        "\n",
        "Newton's method for the vector nonlinear equation $f(x) = 0, f: R^n \\rightarrow R^n$ takes the form\n",
        "\n",
        "$$\n",
        "x^{(k+1)} = x^{(k)} - f'(x^{(k)})^{-1} f(x^{(k)}),\n",
        "$$\n",
        "where $f'(x)$ is the Jacobian matrix of $f$ at $x$.\n",
        "\n",
        "The construction of the inverse of $f'(x)$ is costly, but can be approximated by solving the linear system of equations $f'(x^{(k)}) \\Delta x = -f(x^{(k)})$. In the implementation, finite difference approximation is used to compute the Jacobian matrix, and Gauss-Seidel iteration is employed to solve the linear system.\n",
        "\n",
        "In the implementation, besides `f`, another parameter `x0: np.array` is used to indicate $f$'s domain, as well as work as the initial guess of `x`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kn6UJuf5WqNf"
      },
      "source": [
        "def jacobian(f, x, m, n, h):\n",
        "    J = np.zeros((m, n))\n",
        "    for j in range(n):\n",
        "        dx = np.zeros(n)\n",
        "        dx[j] = h/2\n",
        "        Df = (f(x+dx) - f(x-dx)) / h\n",
        "        J[:,j] = Df\n",
        "    return J\n",
        "\n",
        "\n",
        "def newton_method_vector(f, x0, tol=1e-9, max_iter=2000, h=1e-6):\n",
        "    x = x0.copy()\n",
        "    n_iter = 0\n",
        "    while True:\n",
        "        f_x = f(x)\n",
        "        if norm(f_x) < tol:\n",
        "            return x\n",
        "        J = jacobian(f, x, f_x.shape[0], x.shape[0], h)\n",
        "        x -= gauss_seidel_iteration(J, f_x)\n",
        "        n_iter += 1\n",
        "        if n_iter > max_iter:\n",
        "            raise ValueError('the iteration diverges')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsQLT38gVbn_"
      },
      "source": [
        "# **Results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLwlnOzuV-Cd"
      },
      "source": [
        "In this section, we test the implementations of the four functions.\n",
        "<!-- The exact solutions of equations are obtained by using NumPy's solver. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XizJypAaDrw"
      },
      "source": [
        "## 1. Jacobi iteration for Ax=b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtjPbnp2bAbU"
      },
      "source": [
        "We test the convergence of residual $\\Vert Ax-b \\Vert$, $\\Vert x-y \\Vert$ for manufactured/exact solution $y$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OX2xOIJOmOIF"
      },
      "source": [
        "class TestJacobiIteration(unittest.TestCase):\n",
        "    def test_correctness(self):\n",
        "        A = np.array(\n",
        "            [[2,1,0],\n",
        "             [-1,3,-1],\n",
        "             [0,1,2]])\n",
        "        b = np.array([4,-10,0])\n",
        "        x = jacobi_iteration(A, b)\n",
        "        y = np.array([3,-2,1])\n",
        "        self.assertTrue(norm(A@x - b) / norm(b) < 1e-9)\n",
        "        np.testing.assert_almost_equal(x, y)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NnStm73aG5x"
      },
      "source": [
        "## 2. Gauss-Seidel iteration for Ax=b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNA1KLUGbM7A"
      },
      "source": [
        "We test the convergence of residual $\\Vert Ax-b \\Vert$, $\\Vert x-y \\Vert$ for manufactured/exact solution $y$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACz8M492aKPL"
      },
      "source": [
        "class TestGaussSeidelIteration(unittest.TestCase):\n",
        "    def test_correctness(self):\n",
        "        A = np.array(\n",
        "            [[3,1,-1],\n",
        "             [2,-5,2],\n",
        "             [1,6,8]])\n",
        "        b = np.array([1,9,3])\n",
        "        x = gauss_seidel_iteration(A, b)\n",
        "        y = np.array([1,-1,1])\n",
        "        self.assertTrue(norm(A@x - b) / norm(b) < 1e-9)\n",
        "        np.testing.assert_almost_equal(x, y)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "140PM2Q3aKny"
      },
      "source": [
        "## 3. Newton's method for scalar nonlinear equation f(x)=0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBfLepRvbQxC"
      },
      "source": [
        "We test the convergence of residual $|f(x)|$, $|x-y|$ for manufactured/exact solution $y$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBAp_zh5vmgy"
      },
      "source": [
        "class TestNewtonMethodScalar(unittest.TestCase):\n",
        "    def test_correcness(self):\n",
        "        def f(x):\n",
        "            return (x+1) * (x-1) * (x-3)\n",
        "        x = newton_method(f, x0=3.5)\n",
        "        y = 3\n",
        "        self.assertTrue(abs(f(x)) < 1e-9)\n",
        "        self.assertAlmostEqual(x, y)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGbtk0TSaMO-"
      },
      "source": [
        "## 4. Newton's method for vector nonlinear equation f(x)=0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS3jANhbbZFG"
      },
      "source": [
        "We test the convergence of residual $\\Vert f(x) \\Vert$, $\\Vert x-y \\Vert$ for manufactured/exact solution $y$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI1t-Rtvwd4i"
      },
      "source": [
        "class TestNewtonMethodVector(unittest.TestCase):\n",
        "    def test_correcness(self):\n",
        "        def f(x):\n",
        "            return (x+1) * (x-1) * (x-3)\n",
        "        x = newton_method_vector(f, x0=np.array([-1.25, 1.25, 3.5]))\n",
        "        y = np.array([-1.0, 1.0, 3.0])\n",
        "        self.assertTrue(norm(f(x)) < 1e-9)\n",
        "        np.testing.assert_almost_equal(x, y)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiuMXe2dmVjT"
      },
      "source": [
        "## Conduct the tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVebB-ZrmXj4",
        "outputId": "47592dce-300f-4091-b5db-9d710b93d5c5"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=[''], verbosity=2, exit=False)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_correctness (__main__.TestGaussSeidelIteration) ... ok\n",
            "test_correctness (__main__.TestJacobiIteration) ... ok\n",
            "test_correcness (__main__.TestNewtonMethodScalar) ... ok\n",
            "test_correcness (__main__.TestNewtonMethodVector) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 4 tests in 0.061s\n",
            "\n",
            "OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4GLBv0zWr7m"
      },
      "source": [
        "# **Discussion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bcsDSoRXHZe"
      },
      "source": [
        "The test results are as expected. It is worth a exploratory study to investigate the conditions under which Jacobi iteration and Gauss-Seidel iteration converge. The convergence of Newton's method depends on how close the initial guess of the solution is to the exact solution, thus requiring a relatively accurate guess."
      ]
    }
  ]
}