{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab2_YinengWang.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johanhoffman/methods-in-computational-science/blob/main/MICS_Direct_Methods_YinengWang.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RgtXlfYO_i7"
      },
      "source": [
        "# **Lab 2: Matrix Factorization**\n",
        "**Yineng Wang**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x_J5FVuPzbm"
      },
      "source": [
        "# **Abstract**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UFTSzW7P8kL"
      },
      "source": [
        "This report includes the implementation and tests of sparse matrix-vector product, QR factorization and direct solver of systems of linear equations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkT8J7uOWpT3"
      },
      "source": [
        "#**About the code**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqQCSr4jdO0L"
      },
      "source": [
        "This report is written by Yineng Wang based on a template by Johan Hoffman."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pdll1Xc9WP0e"
      },
      "source": [
        "# Copyright (C) 2020,2021 Johan Hoffman (jhoffman@kth.se)\n",
        "# Copyright (C) 2021 Yineng Wang (yineng@kth.se)\n",
        "\n",
        "# This file is part of the course DD2363 Methods in Scientific Computing\n",
        "# KTH Royal Institute of Technology, Stockholm, Sweden\n",
        "#\n",
        "# This is free software: you can redistribute it and/or modify\n",
        "# it under the terms of the GNU Lesser General Public License as published by\n",
        "# the Free Software Foundation, either version 3 of the License, or\n",
        "# (at your option) any later version."
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28xLGz8JX3Hh"
      },
      "source": [
        "# **Set up environment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2PYNusD08Wa"
      },
      "source": [
        "To have access to the neccessary modules you have to run this cell. The functions for inner product, matrix multiplication and Euclidean norm implemented in Lab 1 are used here as utility functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw7VlErAX7NS"
      },
      "source": [
        "import unittest\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnO3lhAigLev"
      },
      "source": [
        "# **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5zMzgPlRAF6"
      },
      "source": [
        "This laboratory assignment include the implementation of sparse matrix-vector product, QR factorization and direct solver of systems of linear equations.\n",
        "\n",
        "QR factorization employs modified Gram-Schmidt iteration, and the solver is based on QR factorization.\n",
        "\n",
        "The implementations are based on the lecture notes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOQvukXZq5U5"
      },
      "source": [
        "# **Method**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF4iBj5VURZx"
      },
      "source": [
        "The implementations use NumPy arrays to represent objects in linear algebra (vectors and matrices)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-DWkKCM8hFz"
      },
      "source": [
        "## 1. Sparse Matrix-vector Product\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5tUo_Jy8-hD"
      },
      "source": [
        "Input: vector $x$, sparse (real, quadratic) matrix $A$: CRS arrays val, col_idx, row_ptr\n",
        "\n",
        "Output: matrix-vector product $b=Ax$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL3rTh_B9WS9"
      },
      "source": [
        "The implementation is based on the lecture notes, section 5.8, as well as [the Wikipedia page](https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_row_.28CSR.2C_CRS_or_Yale_format.29).\n",
        "The input matrix is represented by a 3-tuple of `(val, col_idx, row_ptr)`.\n",
        "Like Wikipedia's implementation, the elements in `val` are indexed from 0. Therefore, the first element in `row_ptr` is 0. This differs from the implementation in the lecture notes and is for avoiding index offsetting.\n",
        "\n",
        "To implement the sparse matrix-vector product, we repeatedly compute the inner product of each row of $A$ and $x$, $(a_{i:}, x) = \\sum_{1 \\le j \\le n, a_{ij} \\neq 0} a_{ij} x_j$. The non-zero elements in $a_{i:}$ can be accessed via `val` and `col_idx`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yy_RrTrzCMSE"
      },
      "source": [
        "def sparse_mat_vec_prod(A, x):\n",
        "    if len(A) != 3:\n",
        "        raise ValueError('A must be of the form (val, col_idx, row_ptr).')\n",
        "    val, col_idx, row_ptr = A\n",
        "    nnz = row_ptr[-1]\n",
        "    m = len(row_ptr) - 1\n",
        "\n",
        "    b = [0] * m\n",
        "    for row in range(m):\n",
        "        for idx in range(row_ptr[row], row_ptr[row+1]):\n",
        "            b[row] += val[idx] * x[col_idx[idx]]\n",
        "    return b"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP4YB2z38lGn"
      },
      "source": [
        "## 2. QR Factorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6kGp-xvKLS-"
      },
      "source": [
        "Input: (real quadratic) matrix $A$\n",
        "\n",
        "Output: orthogonal matrix $Q$, upper triangular matrix $R$, such that $A=QR$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddlUuFAgS4n5"
      },
      "source": [
        "The implementation uses modified Gram-Schmidt iteration, and is based on Algoritem 5.3 in the lecture notes.\n",
        "\n",
        "For each iteration, we construct a base vector $v_{:j} = a_{:j} - \\sum_{i=1}^{j-1} (a_{:j}, q_{:i}) q_{:i}$, and by equation (5.6), (5.7) and (5.8) in the lecture notes, $v_{:j} = (\\prod_{i=1}^{j-1} (I - q_{:i} q_{:i}^T)) a_{:j}$.  Each column of $Q$ can be computed as the normalization of $v_{:j}$, namely, $q_{:j} = v_{:j} / \\Vert v_{:j} \\Vert$. Each column vector $r_{:j}$ of $R$ is the coordinate of $a_{:j}$ under the basis $\\{q_j\\}_{j=1}^{n}$, hence is computed as $r_{ij} = (a_{:j}, q_{:i})$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9R-olSAOzqL"
      },
      "source": [
        "def qr_factorization(A):\n",
        "    n = len(A)\n",
        "    Q = np.zeros((n,n))\n",
        "    R = np.zeros((n,n))\n",
        "\n",
        "    for j in range(n):\n",
        "        v = np.copy(A[:,j])\n",
        "        for i in range(j):\n",
        "            R[i,j] = np.dot(Q[:,i], v)    # ith coordinate of A[:,j] under q\n",
        "            v -= R[i,j] * Q[:,i]\n",
        "        R[j,j] = np.linalg.norm(v)\n",
        "        Q[:,j] = v / R[j,j]\n",
        "    return Q, R"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVmoANi18pms"
      },
      "source": [
        "## 3. Direct Solver Ax=b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKQcztmBsusv"
      },
      "source": [
        "Input: (real, quadratic) matrix $A$, vector $b$\n",
        "\n",
        "Output: vector $x=A^{-1}b$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZucZkdaBzXR"
      },
      "source": [
        "If we compute a QR factorization of $A$, we have that\n",
        "\n",
        "$Ax = b \\Leftrightarrow QRx = b \\Leftrightarrow Rx = Q^{-1}b = Q^T b$.\n",
        "\n",
        "Since $R$ is an upper triangular matrix, it is easy to solve the system of linear equations $Rx = Q^T b$. We can apply the backward substitution algorithm (Algorithm 5.2) in the lecture notes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1vF6Ok7Cj9m"
      },
      "source": [
        "def backward_substitution(U, b):\n",
        "    n = len(U)\n",
        "    if len(b) != n:\n",
        "        raise ValueError('The shape of U and b are unaligned.')\n",
        "\n",
        "    x = np.copy(b)\n",
        "    for row in range(n-1, -1, -1):\n",
        "        for col in range(n-1, row, -1):\n",
        "            x[row] -= U[row,col] * x[col]\n",
        "        x[row] /= U[row,row]\n",
        "    return x\n",
        "\n",
        "\n",
        "def direct_solve(A, b):\n",
        "    Q, R = qr_factorization(A)\n",
        "    return backward_substitution(R, np.dot(Q.T, b))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsQLT38gVbn_"
      },
      "source": [
        "# **Results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLwlnOzuV-Cd"
      },
      "source": [
        "The tests are carried out in Python's `unittest` framework. The NumPy's implmentations are used as a comparison to make sure the results are correct.\n",
        "\n",
        "Since there might be round-off errors, NumPy's `assert_almost_equal` is used, which checks up to 7 decimals by default, instead of `assert_equal`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GadVSWdHH-7s"
      },
      "source": [
        "## 1. Sparse Matrix-vector Product"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IrP2rHc8vkp"
      },
      "source": [
        "class TestSparseMatVecProd(unittest.TestCase):\n",
        "    def arg_validation(self):\n",
        "        A = np.array([[1, 2], [0, 1]])\n",
        "        x = np.array([3.0, 4.0])\n",
        "        np.testing.assert_raises(ValueError, sparse_mat_vec_prod, A, x)\n",
        "\n",
        "    def test_correctness(self):\n",
        "        A = np.array([[0, 0, 1.4, 0, 3.1], [2.1, 0, 0, 0, 0], [-7.2, 0, 0, -10, 0]])\n",
        "        A_sparse = (\n",
        "            np.array([1.4, 3.1, 2.1, -7.2, -10]),\n",
        "            np.array([2, 4, 0, 0, 3]),\n",
        "            np.array([0, 2, 3, 5], dtype=int)\n",
        "        )\n",
        "        x = np.array([3.5, -4.23, 1.6, 0.85, 1.4])\n",
        "        p1 = sparse_mat_vec_prod(A_sparse, x)\n",
        "        p2 = np.dot(A, x)\n",
        "        np.testing.assert_almost_equal(p1, p2)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn4mdOdlIiWz"
      },
      "source": [
        "## 2. QR Factorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Z1UMSwJBkuO"
      },
      "source": [
        "For this part, NumPy's implementation `np.linalg.qr` can be different from ours, in that the selection of $Q$ and $R$ is not unique. Therefore, we only test the properties of $Q$ and $R$, namely, $Q$ must be an orthonormal matrix, and $R$ must be an upper triangular matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfCJtF2J8woe"
      },
      "source": [
        "class TestQRFactorization(unittest.TestCase):\n",
        "    def test_correctness(self):\n",
        "        A = np.array([[1.5, 1.0, 8], [5, 2.0, 0.75], [1, 4.1, 0]])\n",
        "        Q, R = qr_factorization(A)\n",
        "        np.testing.assert_almost_equal(A, np.dot(Q, R))    # A = QR\n",
        "        np.testing.assert_almost_equal(np.linalg.inv(Q), Q.T)    # Q^-1 = Q^T\n",
        "        np.testing.assert_almost_equal(R, np.triu(R))    # R is upper triangle"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnb9ZK2aIlpL"
      },
      "source": [
        "## 3. Direct Solver Ax=b"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j67eIPpY8yOA"
      },
      "source": [
        "class TestDirectSolve(unittest.TestCase):\n",
        "    def arg_validation(self):\n",
        "        A = np.array([[1, 2], [0, 1]])\n",
        "        b = np.array([3.0])\n",
        "        np.testing.assert_raises(ValueError, direct_solve, A, b)\n",
        "\n",
        "    def test_correctness(self):\n",
        "        A = np.array([[5, 2.0, -0.75], [1.5, 1, 8], [1, 4.1, 0]])\n",
        "        b = np.array([3.0, -1.75, -2.63])\n",
        "        x1 = direct_solve(A, b)\n",
        "        x2 = np.linalg.solve(A, b)\n",
        "        np.testing.assert_almost_equal(x1, x2)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wHStPc-80CR"
      },
      "source": [
        "## Conduct the test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GibzmNO82Jo",
        "outputId": "aec373cd-d968-4735-ccbf-8059c9e33a1e"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=['no_arg'], exit=False)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "...\n",
            "----------------------------------------------------------------------\n",
            "Ran 3 tests in 0.048s\n",
            "\n",
            "OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4GLBv0zWr7m"
      },
      "source": [
        "# **Discussion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bcsDSoRXHZe"
      },
      "source": [
        "All testing cases are passed. The results are expected, under the assumption that round-off errors less than or equal to 7 decimals are tolerated.\n",
        "\n",
        "According to the lectures notes, the modified Gram-Schmidt algorithm is numerically more stable than the classical Gram-Schmidt algorithm. This might require furthur validation."
      ]
    }
  ]
}
